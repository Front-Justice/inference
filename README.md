# PROJET FRONT JUSTICE

ChaÃ®ne de traitement des minutes dans le cadre du projet **Front Justice**.

## ğŸ”§ PrÃ©requis

**Important** : toutes les opÃ©rations doivent Ãªtre effectuÃ©es depuis lâ€™environnement virtuel.

```bash
source env/bin/activate
```

1. Installation de YALTAi :

```bash
pip install YALTAi
```

ğŸ“ Lien : [https://github.com/ponteineptique/yaltai](https://github.com/ponteineptique/yaltai)

---

## âœï¸ Reconnaissance automatique de lâ€™Ã©criture

Le traitement suit une chaÃ®ne bien dÃ©finie :

* DÃ©tection des zones et des lignes via YALTAi et Kraken
* Reconnaissance des caractÃ¨res avec un modÃ¨le entraÃ®nÃ© (Kraken)
* Correction manuelle via eScriptorium (noms propres, date, numÃ©ro de jugement, armÃ©e)
* Post-traitement et extraction du contenu avec Ollama

---

### ğŸ“ Analyse de la mise en page

1. Se placer dans `htr/dataset`
2. Placer les numÃ©risations dans le dossier `dataset`
3. Convertir les images en `jpg`

```bash
mogrify -format jpg *.tif
```

4. Lancer la dÃ©tection des objets et des lignes :

```bash
yaltai kraken --device cuda:0 -a -I "*.jpg" --suffix ".xml" segment --yolo models/weights.pt -i models/250p-escript.mlmodel
```

ğŸ“ RÃ©sultat : fichiers ALTO `.xml` gÃ©nÃ©rÃ©s Ã  partir des images `.jpg`, avec support GPU.

---

### ğŸ”¡ Reconnaissance des caractÃ¨res

Appliquer le modÃ¨le entraÃ®nÃ© `250p_best.mlmodel` :

```bash
kraken -d cuda:0 -a -I "*.xml" -o ".ocr.xml" -f xml ocr -m models/250p_best.mlmodel
```

ğŸ—‚ RÃ©sultats stockÃ©s dans les fichiers `*.ocr.xml`.

#### â• Traitement des signatures

Remplacer chaque ligne marquÃ©e `LABEL="CustomLine:signature"` par un simple `+` :

```bash
cd htr/scripts
python3 signature.py
```

---

## ğŸ” VÃ©rifications manuelles essentielles

Sur la **premiÃ¨re page de chaque minute**, vÃ©rifier :

* Les noms propres
* La date
* Le numÃ©ro de jugement
* Le nom de lâ€™armÃ©e concernÃ©e

Ces Ã©lÃ©ments critiques ne doivent pas Ãªtre laissÃ©s Ã  la seule reconnaissance automatique.

ğŸ“¦ Pour faciliter lâ€™import dans eScriptorium :

```bash
zip ocr.zip *.ocr.xml
```

AprÃ¨s correction dans eScriptorium, exporter les transcriptions au format `ALTO` dans `dataset/export`.

---

## ğŸ§¹ Post-traitement & structuration

### ğŸ§­ Remise en ordre des lignes

Assurer lâ€™ordre logique des lignes sur la premiÃ¨re page :

```bash
python3 ordre.py
```

### ğŸ†” Nettoyage des ID et des rÃ©gions

* Renommage cohÃ©rent des ID des rÃ©gions et lignes
* Suppression des zones/lignes vides

```bash
python3 net-reg-lig.py
```

### ğŸ“ Structuration finale et extraction du texte

Organise chaque minute dans un dossier dÃ©diÃ© et extrait le texte dans un fichier `min_*.txt` :

```bash
python3 tri+texte.py
```

Ces fichiers serviront Ã  lâ€™analyse NER.

---

## ğŸ¤– Correction via Ollama

TÃ©lÃ©charger Ollama, puis choisir le meilleur modÃ¨le possible en fonction de la puissance de lâ€™ordinateur.
Dans notre cas, il s'agit de **Phi-4**.

Appliquer le modÃ¨le LLM Phi-4 pour corriger automatiquement les transcriptions HTR, y compris les toponymes :

```bash
python3 post-oll.py
```

---

## ğŸ§  Extraction des informations via Ollama

La reconnaissance d'entitÃ©s nommÃ©es (NER) est effectuÃ©e avec notre LLM, qui produit des fichiers JSON :

```bash
python3 ner.py
```

---

## âœ… Prochaines Ã©tapes

1. AmÃ©liorer la qualitÃ© de lâ€™HTR :

   * Atteindre **1 000** images annotÃ©es sur Roboflow
   * Atteindre **500** exemples pour la segmentation (notamment pour dÃ©tecter les marges)
   * EntraÃ®ner un modÃ¨le `Party` pour la reconnaissance, Ã  partir du dataset
     (âš ï¸ nÃ©cessite un GPU puissant)

2. Automatiser la dÃ©tection des **amnisties** et des **remises de peine** dans les minutes.

